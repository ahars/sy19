# UV : SY19 - TP04
# Auteurs : Alice Ngwembou - Antoine Hars
# Exercice 1
# Fichier : ex1_tp4.R

###### EX1 ####################################################################

library(MASS)
library(e1071)

# Points 
x1 = c(2,0)
x2 = c(0,2)
x3 = c(-2,2)
x4 = c(-1,3)

# Matrice des points
obs = matrix(c(2,0,-2,-1,0,2,2,3), ncol=2)

# Vecteur des classes
classes = c(1,-1,-1,-1)
classes = t(classes) 
couleur = c("blue","green", "green", "green")

# Plot des points
png("plots/ex1_hyperplan.png")
plot(obs, col = couleur, xlim = c(-3,3), ylim = c(-2,3), xlab = "", ylab ="", main = "Données d'apprentissage et hyperplan séparateur")
abline(0,1, col = "red", lwd=3)
abline(2,1, col = "green", lwd=1)
abline(-2,1, col = "blue", lwd=1)
text(0.35,0, expression(x == y), col = "red")
text(-2.65,0, expression(y == x + 2), col = "green")
text(2.65,0, expression(y == x - 2 ), col = "blue")
dev.off()

# Méthode SVM
model = svm(obs,as.factor(classes), scale = FALSE, type = NULL, kernel =
"linear")

# Affichage des vecteurs supports
model$SV

# Affichage des coefficients de lagrange*traininglevels
model$coefs

###### EX3 ####################################################################

# Points 
x1 = 1
x2 = 2
x3 = 6
x4 = 4
x5 = 5

# Classes
classes = c(1,1,1,-1,-1)
classes = t(classes) 
couleur = c("green", "green", "green", "blue", "blue")

# Matrice des points
obs = matrix(c(1,2,6,4,5))

# Methode SVM
model = svm(obs,as.factor(classes),degree = 2, scale=FALSE, kernel="polynomial", coef0=1, cost=100, gamma=1)

# Question 5

K <- function(xi,x){
	return((xi*x+1)^2)
}
g <- function(model,obs,x)
{	n = length(x)
	r = c(1:n)
	for (i in 1:n) 
	{
		r[i] <- model$coefs[1]*K(obs[model$index[1]], x[i])+model$coefs[2]*K(obs[model$index[2]], x[i])+model$coefs[3]*K(obs[model$index[3]], x[i])-model$rho
	}
	return (r)
}
g2 <- function(x)
{	
	classes = c(1,1,1,-1,-1)
	classes = t(classes) 
	obs = matrix(c(1,2,6,4,5))
	model = svm(obs,as.factor(classes),degree = 2, scale=FALSE, kernel="polynomial", coef0=1, cost=100, gamma=1)

	n = length(x)
	r = c(1:n)
	for (i in 1:n) 
	{
		r[i] <- model$coefs[1]*K(obs[model$index[1]], x[i])+model$coefs[2]*K(obs[model$index[2]], x[i])+model$coefs[3]*K(obs[model$index[3]], x[i])-model$rho
	}
	return (r)
}

x = seq(from = 0, to = 7, by = 0.5)
res = g(model,obs,x)
png("plots/ex3_graphe.png")
plot(x,res, type = 'l',col ="pink",lwd=3, xlab = "x", ylab ="g(x)", main = "Visualisation de la fonction d'apprentissage g(x)")
points(obs,classes, col = "black", lwd = 2)
points(obs[2],classes[2], col = "red", lwd=4)
points(obs[3],classes[3], col = "red", lwd=4)
points(obs[5],classes[5], col = "red", lwd=4)
abline(1,0, col = "green", lwd=1)
abline(-1,0, col = "blue", lwd=1)
abline(-1,0, col = "blue", lwd=1)
text(0.5,1.5, expression(g(x) == 1), col = "green")
text(0.5,-0.5, expression(g(x) == -1), col = "blue")
dev.off()

png("plots/ex3_graphe.png")
plot(x,res, type = 'l',col ="green",lwd=3, xlab = "x", ylab ="g(x)", main = "Visualisation de la fonction d'apprentissage g(x)")
abline(0,0, col = "black", lwd=1)
text(1,6, expression(g(x)), col = "green")
text(0.2,0.5, expression(axe_x), col = "black")
points(obs[1],0, col = "red", lwd=1)
points(obs[2],0, col = "red", lwd=5)
points(obs[3],0, col = "red", lwd=5)
points(obs[4],0, col = "blue", lwd=1)
points(obs[5],0, col = "blue", lwd=5)
points(5.58,0, col = "cyan", lwd=2)
points(2.42,0, col = "cyan", lwd=2)
abline(v = 5.58, col = "cyan")
abline(v = 2.42, col = "cyan")
text(3,6, expression(x == 2.42), col = "cyan")
text(6.2,6, expression(x == 5.58), col = "cyan")
dev.off()

uniroot(g2, interval=c(0, 4))
uniroot(g2, interval=c(4, 7))

library(MASS)
install.packages(repos = NULL, pkgs = "e1071_1.6-1.tar.gz")
library(e1071)

##### EX4 ###########################################

# lire " breast cancer data " :
bcdata <- read.csv("breast-cancer-wisconsin.data", head = TRUE)

# Regarder les noms des colonnes.
names(bcdata)

# regarder une colonne, par exemple:
bcdata$ClumpThickness # ou bcdata$Samplecodenumber

# regarder les classes:
bcdata$Class

# Enlever de bcdata la colonne "Samplecodenumber" et la colonne
# "Class". Le signe moins pour dire unselect les deux colonnes:
databcall <- subset(bcdata, select = c(-Samplecodenumber, -Class))
# vous pouvez faire names(databcall)

# Selectionner les classes seulement:
classesbcall <- subset(bcdata, select = Class)

# prendre une partie de databcall pour l’apprentissage:
databctrain <- databcall[1:400,]
classesbctrain <- classesbcall[1:400,]

# prendre une partie de databcall pour le test:
databctest <- databcall[401:699,]
classesbctest <- classesbcall[401:699,]

##################################################

##################################################

# Faire help(svm) pour bien comprendre la fonction "svm" :
model <- svm(databctrain, classesbctrain)

# Faire str(model)
# Faire help(predict.svm) #Attention, ici "model" est un objet de
# la classe "svm". Ainsi, il faut faire help(predict.svm) et non pas
# help(predict)
# Validation de "model" :
pred <- predict(model, databctest)

# Comparer la prediction et les vraies classes :
table(pred, t(classesbctest))

# Les hyperparametres affecte les performances du noyau. Le
# package e1071 offre une fonction, tune(), qui fait ce qu’on
# appelle "grid search" et donne ainsi une estimation des
# parametres. Pour cela, faire : help(tune)
# Exemple d’application de la fonction "tune" :
a = tune(svm, train.x = databctrain, train.y = classesbctrain, validation.x = databctest, validation.y = classesbctest, ranges = list(gamma = 10^(-1:1), cost = c(1,1.5,2)), control = tune.control(sampling = "fix"))

# Faire str(a)
model <- svm(databctrain, classesbctrain, gamma = a$best.parameters$gamma, cost = a$best.parameters$cost)

# Comparaison de la prediction et des vraies classes
pred <- predict(model, databctest)
table(pred, t(classesbctest))
##################################################

# 2.
b = c()
for(i in 1:100) {
	ta <- table(predict(svm(databctrain, classesbctrain, gamma = a$best.parameters$gamma, cost = i, kernel = "radial"), databctest), t(classesbctest))
	b <- cbind(b, c(round(i, digits = 3), ((ta[1,2] + ta[2,1]) / sum(ta)) * 100))
}

png("plots/q2_proba_erreur_gamma.png")
plot(t(b), main = "Variation proba d’erreur en fonction de gamma", xlab = "largeur de bande de gamma", ylab = "la probabilité d'erreur", col = "red")
dev.off()

# composantes du vecteur alpha
png("plots/q2_boxplot_alpha_.png")
par(mfrow = c(2, 5))
for(i in 1:10) {
	boxplot(svm(databctrain, classesbctrain, gamma = a$best.parameters$gamma, cost = i^2, kernel = "radial")$coefs, main = round(i^2, digits = 3))
}
dev.off()

b = c()
for(i in 1:100) {
	b <- cbind(b, c(round(i, digits = 3), dim(svm(databctrain, classesbctrain, gamma = a$best.parameters$gamma, cost = i, kernel = "radial")$coefs)[1]))
}
png("plots/q2_alpha_positifs.png")
plot(t(b), xlab = "largeur de bande de gamma", ylab = "le nombre d'alpha non nuls", main = "Variation des alpha positifs en fonction de gamma", col = "red")
dev.off()

# 3.
# gamma = 100 = infini
b = c()
for(i in 1:100) {
	ta <- table(predict(svm(databctrain, classesbctrain, gamma = i / 100, cost = 100, kernel = "radial"), databctest), t(classesbctest))
	b <- cbind(b, c(i / 100, ((ta[1,2] + ta[2,1]) * 100) / sum(ta)))
}

png("plots/q3_proba_erreur_gamma1.png")
plot(t(b), xlab = "largeur de bande de gamma (intervalle [0;1])", ylab = "la probabilité d'erreur", main = "Variation proba d'erreur en fonction de gamma", col = "red")
dev.off()

# probabilité d'erreur en fonction du paramètre de pénalisation 
ba = c()
for (i in 1:50) {
	ta <- table(predict(svm(databctrain, classesbctrain, gamma = i / 2, cost = 100, kernel = "radial"), databctest), t(classesbctest))
	ba <- cbind(ba, c(i / 2, ((ta[1,2] + ta[2,1]) * 100) / sum(ta)))
}
png("plots/q3_proba_erreur_gamma2.png")
plot(t(ba), xlab = "largeur de bande de gamma (intervalle [0;25])", ylab = "la probabilité d'erreur", main = "Variation proba d'erreur en fonction de gamma", col = "red")
dev.off()

b = c()
for(i in 1:50) {
	b <- cbind(b, c(round(i / 50, digits = 3), dim(svm(databctrain, classesbctrain, gamma = i / 50, cost = 100, kernel = "radial")$coefs)[1]))
}
png("plots/q3_alpha_positifs.png")
plot(t(b), xlab = "largeur de bande de gamma", ylab = "le nombre d'alpha non nuls", main = "Variation des alpha positifs en fonction de gamma", col = "red")
dev.off()

# composantes du vecteur alpha
png("plots/q3_boxplot_alpha_.png")
par(mfrow = c(1, 5))
for(i in 1:5) {
	boxplot(svm(databctrain, classesbctrain, gamma = i / 50, cost = 100, kernel = "radial")$coefs, main = round(i / 20, digits = 3))
}
dev.off()

# 4.
a = tune(svm, train.x = databctrain, train.y = classesbctrain, validation.x = databctest, validation.y = classesbctest, ranges = list(gamma = 1, cost = c(1:100)), control = tune.control(sampling = "fix"))

b = c()
# gaussien
model <- svm(databctrain, classesbctrain, gamma = a$best.parameters$gamma, cost = a$best.parameters$cost, kernel = "radial")
pred <- predict(model, databctest)
ta <- table(pred, t(classesbctest))
b <- cbind(b, c(1, ((ta[1,2] + ta[2,1]) * 100) / sum(ta)))	
png("plots/q4_gaussien.png")
plot(t(b), main = "", xlab = "", ylab = "")
dev.off()

# polynomial
for(i in 1:15) {
	model <- svm(databctrain, classesbctrain, gamma = a$best.parameters$gamma, cost = a$best.parameters$cost, coef0 = 1, degree = i, kernel = "polynomial")
	pred <- predict(model, databctest)
	ta <- table(pred, t(classesbctest))
	b <- cbind(b, c(2, ((ta[1,2] + ta[2,1]) * 100) / sum(ta)))
}
png("plots/q4_polynomial.png")
plot(t(b), main = "", xlab = "", ylab = "")
dev.off()

